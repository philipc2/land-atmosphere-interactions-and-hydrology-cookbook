{"version":"1","records":[{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook"},"type":"lvl1","url":"/#digital-earths-global-hackathon-land-atmosphere-and-hydrology-applications-cookbook","position":2},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook"},"content":"\n\n\n\n\n\n\n\nThis Project Pythia Cookbook contains a collection of computational notebooks for land-atmosphere and hydrology applications from the NSF NCAR node of the 2025 Digital Earths Global Hackathon.","type":"content","url":"/#digital-earths-global-hackathon-land-atmosphere-and-hydrology-applications-cookbook","position":3},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl2":"Authors"},"content":"Erik Janzon, \n\nYifan Cheng, \n\nAashish Panta, and \n\nKatelyn FitzGerald","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl2":"Structure"},"content":"Includes our work in progress notebooks for the time being.","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":10},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":11},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":12},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":13},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":14},{"hierarchy":{"lvl1":"Digital Earths Global Hackathon: Land-Atmosphere and Hydrology Applications Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/land-atmosphere-interactions-and-hydrology-cookbook repository: git clone https://github.com/ProjectPythia/land-atmosphere-interactions-and-hydrology-cookbook.git\n\nMove into the land-atmosphere-interactions-and-hydrology-cookbook directorycd land-atmosphere-interactions-and-hydrology-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate land-atmosphere-interactions-and-hydrology-cookbook-dev\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":15},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using icon_d3hp003"},"type":"lvl1","url":"/calc-pet-coupling-icon-d3hp003","position":0},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using icon_d3hp003"},"content":"","type":"content","url":"/calc-pet-coupling-icon-d3hp003","position":1},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using icon_d3hp003"},"type":"lvl1","url":"/calc-pet-coupling-icon-d3hp003#calculate-the-sm-t-coupling-using-icon-d3hp003","position":2},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using icon_d3hp003"},"content":"Author: Yifan Cheng\n\nReference:\n\nCoupling​_metrics​_V30​_SM​-T​.pdf\n\nMiralles et al. (2012)\n\nimport datetime\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\nprint(f'Last updated at {current_time}')\n\nfrom pathlib import Path \nimport xarray as xr\nimport cartopy.crs as ccrs\nimport uxarray as ux\nimport numpy as np\nimport cartopy.feature as cf\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport intake\n\ncat_url = \"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"\ncat = intake.open_catalog(cat_url).NCAR\n\n# check on global models\nmodel_run1 = cat['icon_d3hp003']\nmodel_run2 = cat['ifs_tco3999-ng5_deepoff']\nmodel_run3 = cat['nicam_gl11']\nmodel_run4 = cat['mpas_dyamond3']\nmodel_run5 = cat['mpas_dyamond2']\nmodel_run6 = cat['scream_ne120'] #3hourly average\n\nds_fine1 = model_run1(zoom=7,time='P1D').to_dask()\n\nuxds_fine1 = ux.UxDataset.from_healpix(ds_fine1)\n\nvar_longname_dict1 = {\n    var: ds_fine1[var].attrs.get(\"long_name\", \"N/A\")\n    for var in ds_fine1.data_vars\n}\n\nvar_longname_dict1\n\n# 'pres_sfc': 'surface pressure'\n#  'rlds': 'surface downwelling longwave radiation',\n#  'rlus': 'surface upwelling longwave radiation'\n# 'rsds': 'surface downwelling shortwave radiation',\n#  'rsus': 'surface upwelling shortwave radiation',\n# 'hfls': 'latent heat flux',\n#  'hfss': 'sensible heat flux',\n# 'tas': 'temperature in 2m',\n\nland_mask=ux.UxDataArray((uxds_fine1.sftlf==1).astype(int),uxgrid=uxds_fine1.uxgrid)\n\nland_mask.plot.points(\n    cmap=\"inferno\",\n    rasterize=True,\n    dynamic=False,\n    features=[\"coastline\", \"borders\"]\n)\n\nuxds_fine1=uxds_fine1.sel(time=uxds_fine1.time.dt.year==2020)\n\n%%time\nsel_vswc=(uxds_fine1['mrso'])\nsel_le=(uxds_fine1['hflsd']) #W/m2\nsel_h=(uxds_fine1['hfssd']) #W/m2\nsel_rnet=((uxds_fine1['rlds']-uxds_fine1['rlus']+uxds_fine1['rsds']-uxds_fine1['rsus'])) #W/m2\nsel_t2m=(uxds_fine1['tas']-273.15) #C\nsel_sp=(uxds_fine1['ps']/1000) #kPa\n\n%%time\nsel_le_DJF=sel_le.sel(time=(uxds_fine1.time.dt.month.isin([1,2,12]))&(uxds_fine1.time.dt.year == 2020)).where(land_mask)\nsel_h_DJF=sel_h.sel(time=(uxds_fine1.time.dt.month.isin([1,2,12]))&(uxds_fine1.time.dt.year == 2020)).where(land_mask)\nsel_rnet_DJF=sel_rnet.sel(time=(uxds_fine1.time.dt.month.isin([1,2,12]))&(uxds_fine1.time.dt.year == 2020)).where(land_mask)\nsel_t2m_DJF=sel_t2m.sel(time=(uxds_fine1.time.dt.month.isin([1,2,12]))&(uxds_fine1.time.dt.year == 2020)).where(land_mask)\nsel_sp_DJF=sel_sp.sel(time=(uxds_fine1.time.dt.month.isin([1,2,12]))&(uxds_fine1.time.dt.year == 2020)).where(land_mask)\n\n%%time\nsel_le_JJA=sel_le.sel(time=(uxds_fine1.time.dt.month.isin([6,7,8])&(uxds_fine1.time.dt.year.isin([2020])))).where(land_mask)\nsel_h_JJA=sel_h.sel(time=(uxds_fine1.time.dt.month.isin([6,7,8])&(uxds_fine1.time.dt.year.isin([2020])))).where(land_mask)\nsel_rnet_JJA=sel_rnet.sel(time=(uxds_fine1.time.dt.month.isin([6,7,8])&(uxds_fine1.time.dt.year.isin([2020])))).where(land_mask)\nsel_t2m_JJA=sel_t2m.sel(time=(uxds_fine1.time.dt.month.isin([6,7,8])&(uxds_fine1.time.dt.year.isin([2020])))).where(land_mask)\nsel_sp_JJA=sel_sp.sel(time=(uxds_fine1.time.dt.month.isin([6,7,8])&(uxds_fine1.time.dt.year.isin([2020])))).where(land_mask)\n\ndef saturation_vapor_pressure(T):\n    return 6.112*np.exp((17.67*T)/(T+243.5))*0.1  # hPa -> kPa\n\ndef delta_svp(T):\n    es=saturation_vapor_pressure(T)\n    return (4302.645*es)/((T+243.5)**2)  # in kPa/°C\n\ndef alpha_piecewise(vswc): # values from literature based on Priestley–Taylor equation\n    return xr.where(\n        vswc < 0.1, 0.8,\n        xr.where(vswc < 0.2, 1.0, 1.26)\n    )\n\ndef calc_gamma(P_kPa):\n    cp=1.013e3      # J/kg/°C\n    epsilon=0.622\n    lambda_v=2.45e6 # Latent heat of vaporization J/kg\n    return (cp*P_kPa)/(epsilon*lambda_v)  # in kPa/°C\n\ndef priestley_taylor_ET(vswc, Rnet, T, P_kPa): # assume Rnet=LE+H\n    alpha=alpha_piecewise(vswc)\n    delta=delta_svp(T)\n    gamma=calc_gamma(P_kPa)\n    coeff=alpha*(delta/(delta+gamma))\n    return coeff*Rnet  # W/m2\n\ndef safe_corr(x, y, dim='time', min_count=30):\n    valid = x.notnull() & y.notnull()\n    count = valid.sum(dim=dim)\n    r = xr.corr(x, y, dim=dim)\n    return r.where(count >= min_count)\n    \ndef calculate_coupling_index(vswc, Rnet, LE, T, P):\n    H=Rnet-LE\n\n    LE_p=priestley_taylor_ET(vswc, Rnet, T, P)\n    H_p=Rnet-LE_p\n    \n    H_xr=H.to_dataset(name='H')\n    Hp_xr=H_p.to_dataset(name='Hp')\n    T_xr=T.to_dataset(name='T2M')\n\n    H_corr = safe_corr(H, T, dim='time', min_count=30)\n    Hp_corr = safe_corr(H_p, T, dim='time', min_count=30)\n\n    PI=H_corr-Hp_corr\n    PI.name='coupling_index'\n\n    return PI\n\n%%time\nPI_DJF=calculate_coupling_index(sel_rnet_DJF,sel_le_DJF,sel_t2m_DJF,sel_sp_DJF).compute()\n\nPI_JJA=calculate_coupling_index(sel_rnet_JJA,sel_le_JJA,sel_t2m_JJA,sel_sp_JJA).compute()\n\nPI_DJF.plot()\n\nPI_JJA.plot()","type":"content","url":"/calc-pet-coupling-icon-d3hp003#calculate-the-sm-t-coupling-using-icon-d3hp003","position":3},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using ifs_tco3999-ng5_rcbmf_cf"},"type":"lvl1","url":"/calc-pet-coupling-ifs-tco3999-ng5-rcbmf-cf","position":0},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using ifs_tco3999-ng5_rcbmf_cf"},"content":"","type":"content","url":"/calc-pet-coupling-ifs-tco3999-ng5-rcbmf-cf","position":1},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using ifs_tco3999-ng5_rcbmf_cf"},"type":"lvl1","url":"/calc-pet-coupling-ifs-tco3999-ng5-rcbmf-cf#calculate-the-sm-t-coupling-using-ifs-tco3999-ng5-rcbmf-cf","position":2},{"hierarchy":{"lvl1":"Calculate the SM-T coupling using ifs_tco3999-ng5_rcbmf_cf"},"content":"Author: Yifan Cheng\n\nReference:\n\nCoupling​_metrics​_V30​_SM​-T​.pdf\n\nMiralles et al. (2012)\n\nimport datetime\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\nprint(f'Last updated at {current_time}')\n\nfrom pathlib import Path \nimport xarray as xr\nimport cartopy.crs as ccrs\nimport uxarray as ux\nimport numpy as np\nimport cartopy.feature as cf\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport intake\n\ncat_url = \"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"\ncat = intake.open_catalog(cat_url).NCAR\n\n# check on global models\nmodel_run2 = cat['ifs_tco3999-ng5_rcbmf_cf']\n\nds_fine2 = model_run2(zoom=7,time='PT1H').to_dask()\n\nuxds_fine2 = ux.UxDataset.from_healpix(ds_fine2)\n\n# for var_name, da in uxds_fine2.data_vars.items():\n#     if da.dims == ('time', 'n_face') and da.isel(time=0).isnull().all():\n#         print(f\"Variable '{var_name}' is all NaN\")\n\nvar_longname_dict2 = {\n    var: ds_fine2[var].attrs.get(\"name\", \"N/A\")\n    for var in ds_fine2.data_vars\n}\n\nvar_longname_dict2\n\n#slhf #Surface latent heat flux\n#sshf #Sensible heat flux\n#ssr #Surface net short-wave (solar) radiation\n#str #Surface net long-wave (thermal) radiation\n#2t #2 metre temperatur\n\nland_mask=ux.UxDataArray((uxds_fine2.sst.isel(time=0)==9999).astype(int),uxgrid=uxds_fine2.uxgrid)\n\nsel_vswc=(uxds_fine2['mrso']).where(land_mask)\nsel_le=(uxds_fine2['hflsd']).where(land_mask) #W/m2\nsel_h=(uxds_fine2['hfssd']).where(land_mask) #W/m2\nsel_rnet=((uxds_fine2['rlds']-uxds_fine2['rlus']+uxds_fine2['rsds']-uxds_fine2['rsus'])).where(land_mask) #W/m2\nsel_t2m=(uxds_fine2['tas']-273.15).where(land_mask) #C\nsel_sp=(uxds_fine2['ps']/1000).where(land_mask) #kPa\n\nsel_vswc_DJF=sel_vswc.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020))\nsel_le_DJF=sel_le.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020))\nsel_h_DJF=sel_h.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020))\nsel_rnet_DJF=sel_rnet.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020))\nsel_t2m_DJF=sel_t2m.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020))\nsel_sp_DJF=sel_sp.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020))\n\nsel_vswc_JJA=sel_vswc.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020]))))\nsel_le_JJA=sel_le.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020]))))\nsel_h_JJA=sel_h.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020]))))\nsel_rnet_JJA=sel_rnet.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020]))))\nsel_t2m_JJA=sel_t2m.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020]))))\nsel_sp_JJA=sel_sp.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020]))))\n\ndaily_sel_vswc_DJF=sel_vswc_DJF.resample(time='1D').mean()\ndaily_sel_le_DJF=sel_le_DJF.resample(time='1D').mean()\ndaily_sel_h_DJF=sel_h_DJF.resample(time='1D').mean()\ndaily_sel_rnet_DJF=sel_rnet_DJF.resample(time='1D').mean()\ndaily_sel_t2m_DJF=sel_t2m_DJF.resample(time='1D').mean()\ndaily_sel_sp_DJF=sel_sp_DJF.resample(time='1D').mean()\n\ndaily_sel_vswc_JJA=sel_vswc_JJA.resample(time='1D').mean()\ndaily_sel_le_JJA=sel_le_JJA.resample(time='1D').mean()\ndaily_sel_h_JJA=sel_h_JJA.resample(time='1D').mean()\ndaily_sel_rnet_JJA=sel_rnet_JJA.resample(time='1D').mean()\ndaily_sel_t2m_JJA=sel_t2m_JJA.resample(time='1D').mean()\ndaily_sel_sp_JJA=sel_sp_JJA.resample(time='1D').mean()\n\ndaily_sel_le_JJA.shape\n\ndef saturation_vapor_pressure(T):\n    return 6.112*np.exp((17.67*T)/(T+243.5))*0.1  # hPa -> kPa\n\ndef delta_svp(T):\n    es=saturation_vapor_pressure(T)\n    return (4302.645*es)/((T+243.5)**2)  # in kPa/°C\n\ndef alpha_piecewise(vswc): # values from literature based on Priestley–Taylor equation\n    return xr.where(\n        vswc < 0.1, 0.8,\n        xr.where(vswc < 0.2, 1.0, 1.26)\n    )\n\ndef calc_gamma(P_kPa):\n    cp=1.013e3      # J/kg/°C\n    epsilon=0.622\n    lambda_v=2.45e6 # Latent heat of vaporization J/kg\n    return (cp*P_kPa)/(epsilon*lambda_v)  # in kPa/°C\n\ndef priestley_taylor_ET(vswc, Rnet, T, P_kPa): # assume Rnet=LE+H\n    alpha=alpha_piecewise(vswc)\n    delta=delta_svp(T)\n    gamma=calc_gamma(P_kPa)\n    coeff=alpha*(delta/(delta+gamma))\n    return coeff*Rnet  # W/m2\n\ndef safe_corr(x, y, dim='time', min_count=30):\n    valid = x.notnull() & y.notnull()\n    count = valid.sum(dim=dim)\n    r = xr.corr(x, y, dim=dim)\n    return r.where(count >= min_count)\n    \ndef calculate_coupling_index(vswc, Rnet, LE, T, P):\n    H=Rnet-LE\n\n    LE_p=priestley_taylor_ET(vswc, Rnet, T, P)\n    H_p=Rnet-LE_p\n    \n    H_xr=H.to_dataset(name='H')\n    Hp_xr=H_p.to_dataset(name='Hp')\n    T_xr=T.to_dataset(name='T2M')\n\n    H_corr = safe_corr(H, T, dim='time', min_count=30)\n    Hp_corr = safe_corr(H_p, T, dim='time', min_count=30)\n\n    PI=H_corr-Hp_corr\n    PI.name='coupling_index'\n\n    return PI\n\n%%time\nPI_DJF=calculate_coupling_index(daily_sel_vswc_DJF,daily_sel_rnet_DJF,daily_sel_le_DJF,daily_sel_t2m_DJF,daily_sel_sp_DJF).compute()\n\n%%time\nPI_JJA=calculate_coupling_index(daily_sel_vswc_JJA,daily_sel_rnet_JJA,daily_sel_le_JJA,daily_sel_t2m_JJA,daily_sel_sp_JJA).compute()\n\nPI_JJA.plot()\n\nPI_DJF.plot()","type":"content","url":"/calc-pet-coupling-ifs-tco3999-ng5-rcbmf-cf#calculate-the-sm-t-coupling-using-ifs-tco3999-ng5-rcbmf-cf","position":3},{"hierarchy":{"lvl1":"Calculate the LE-prcp correlation"},"type":"lvl1","url":"/calc-le-prcp-corr","position":0},{"hierarchy":{"lvl1":"Calculate the LE-prcp correlation"},"content":"","type":"content","url":"/calc-le-prcp-corr","position":1},{"hierarchy":{"lvl1":"Calculate the LE-prcp correlation"},"type":"lvl1","url":"/calc-le-prcp-corr#calculate-the-le-prcp-correlation","position":2},{"hierarchy":{"lvl1":"Calculate the LE-prcp correlation"},"content":"Author: Yifan Cheng\n\nReference:\n\nCoupling​_metrics​_V30​_Corr​.pdf\n\nimport datetime\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\nprint(f'Last updated at {current_time}')\n\nfrom pathlib import Path \nimport xarray as xr\nimport cartopy.crs as ccrs\nimport uxarray as ux\nimport numpy as np\nimport cartopy.feature as cf\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n","type":"content","url":"/calc-le-prcp-corr#calculate-the-le-prcp-correlation","position":3},{"hierarchy":{"lvl1":"Identify datasets with SM, P, LE, H etc."},"type":"lvl1","url":"/calc-le-prcp-corr#identify-datasets-with-sm-p-le-h-etc","position":4},{"hierarchy":{"lvl1":"Identify datasets with SM, P, LE, H etc."},"content":"\n\nimport intake\n\ncat_url = \"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"\ncat = intake.open_catalog(cat_url).NCAR\n\n# check on global models\nmodel_run1 = cat['icon_ngc4008']\nmodel_run2 = cat['ifs_tco3999-ng5_deepoff']\nmodel_run3 = cat['nicam_gl11']\nmodel_run4 = cat['mpas_dyamond3']\nmodel_run5 = cat['mpas_dyamond2']\nmodel_run6 = cat['scream_ne120'] #3hourly average\n\nmodel_run1().describe()[\"user_parameters\"]\n\nds_coarsest1 = model_run1().to_dask()\nds_coarsest2 = model_run2().to_dask()\nds_coarsest3 = model_run3().to_dask() \nds_coarsest4 = model_run4().to_dask() \nds_coarsest5 = model_run5().to_dask() \nds_coarsest6 = model_run6().to_dask() \n# ds_fine = model_run(zoom=10,time='PT3H').to_dask()\n\nprint('***available time period (in UTC)***')\nprint('icon_ngc4008',ds_coarsest1.time.min().values,ds_coarsest1.time.max().values)\nprint('ifs_tco3999-ng5_deepoff',ds_coarsest2.time.min().values,ds_coarsest2.time.max().values)\nprint('nicam_gl11',ds_coarsest3.time.min().values,ds_coarsest3.time.max().values)\nprint('mpas_dyamond2',ds_coarsest4.time.min().values,ds_coarsest4.time.max().values)\nprint('mpas_dyamond3',ds_coarsest5.time.min().values,ds_coarsest5.time.max().values)\nprint('scream_ne120',ds_coarsest6.time.min().values,ds_coarsest6.time.max().values)\n\nvar_longname_dict1 = {\n    var: ds_coarsest1[var].attrs.get(\"long_name\", \"N/A\")\n    for var in ds_coarsest1.data_vars\n}\nvar_longname_dict2 = {\n    var: ds_coarsest2[var].attrs.get(\"name\", \"N/A\")\n    for var in ds_coarsest2.data_vars\n}\nvar_longname_dict3 = {\n    var: ds_coarsest3[var].attrs.get(\"long_name\", \"N/A\")\n    for var in ds_coarsest3.data_vars\n}\nvar_longname_dict4 = {\n    var: ds_coarsest4[var].attrs.get(\"long_name\", \"N/A\")\n    for var in ds_coarsest4.data_vars\n}\nvar_longname_dict5 = {\n    var: ds_coarsest5[var].attrs.get(\"long_name\", \"N/A\")\n    for var in ds_coarsest5.data_vars\n}\nvar_longname_dict6 = {\n    var: ds_coarsest6[var].attrs.get(\"long_name\", \"N/A\")\n    for var in ds_coarsest6.data_vars\n}\n\n# MPAS-DYAMOND1/2 (model5) doesn't have latent heat flux\nle_vars=['hfls','slhf','hflsd','lh_tavg','hflsd']\n\n","type":"content","url":"/calc-le-prcp-corr#identify-datasets-with-sm-p-le-h-etc","position":5},{"hierarchy":{"lvl1":"Look into finest spatial res + lowest temporal res"},"type":"lvl1","url":"/calc-le-prcp-corr#look-into-finest-spatial-res-lowest-temporal-res","position":6},{"hierarchy":{"lvl1":"Look into finest spatial res + lowest temporal res"},"content":"\n\nds_fine1 = model_run1(zoom=10,time='PT3H').to_dask()\nds_fine2 = model_run2(zoom=7,time='PT1H').to_dask()\nds_fine3 = model_run3(zoom=9,time='PT3H').to_dask() #no zoomed-in data at PT6H\nds_fine4 = model_run4(zoom=10).to_dask() #hourly\nds_fine6 = model_run6(zoom=7).to_dask() \n\n#ds_fine2 = model_run2(zoom=11,time='PT1H').to_dask()\nds_fine2=ds_fine2.rename_dims({'value': 'cell'}) #IFS models require renaming for healpix to work\n\n%%time\nuxds_fine1 = ux.UxDataset.from_healpix(ds_fine1)\nuxds_fine2 = ux.UxDataset.from_healpix(ds_fine2)\nuxds_fine3 = ux.UxDataset.from_healpix(ds_fine3)\nuxds_fine4 = ux.UxDataset.from_healpix(ds_fine4)\nuxds_fine6 = ux.UxDataset.from_healpix(ds_fine6)\n\n#uxda_fine1 #W/m2\n#uxda_fine2 #J/m2 1hour -> 3600S ->1/3600W/m2\n#uxda_fine3 #W/m2\n#uxda_fine4 #W/m2\n#uxda_fine6 #s^-3 kg\n\n","type":"content","url":"/calc-le-prcp-corr#look-into-finest-spatial-res-lowest-temporal-res","position":7},{"hierarchy":{"lvl1":"Coupling metrics"},"type":"lvl1","url":"/calc-le-prcp-corr#coupling-metrics","position":8},{"hierarchy":{"lvl1":"Coupling metrics"},"content":"\n\n","type":"content","url":"/calc-le-prcp-corr#coupling-metrics","position":9},{"hierarchy":{"lvl1":"Coupling metrics","lvl2":"Basic: correlation"},"type":"lvl2","url":"/calc-le-prcp-corr#basic-correlation","position":10},{"hierarchy":{"lvl1":"Coupling metrics","lvl2":"Basic: correlation"},"content":"\n\ntimestep=18\ntime_utc=pd.to_datetime(str(uxda_fine2.time.values[timestep]))\nif time_utc.tzinfo is None:\n    time_utc = time_utc.tz_localize('UTC')\n\ntime_mdt = time_utc.tz_convert('US/Mountain')\ntime_mdt=time_mdt.strftime('%Y.%m.%d, %H:%M:%S')\n(uxds_fine2['2t']).isel(time=timestep).plot.points(\n    cmap=\"inferno\",\n    rasterize=True,\n    dynamic=False,\n    title=f\"model=ifs_tco3999-ng5_deepoff, time={time_mdt} MDT\",\n    features=[\"coastline\", \"borders\"]\n)\n\n#ifs model\nRnet=uxds_fine2.ssr+uxds_fine2.str # J/m2\nH=uxds_fine2.sshf #real Surface sensible heat flux, J/m2\n\nland_mask=ux.UxDataArray(uxds_fine2.sst.isel(time=0).isnull().astype(int),uxgrid=uxds_fine2.uxgrid)\n\nland_mask.plot.points(\n    cmap=\"inferno\",\n    rasterize=True,\n    dynamic=False,\n    features=[\"coastline\", \"borders\"]\n)\n\nsel_le_DJF=uxds_fine2.slhf.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020)).where(land_mask)/3600 #convert back to W/m2\nsel_tp_DJF=uxds_fine2.tp.sel(time=(uxds_fine2.time.dt.month.isin([1,2,12]))&(uxds_fine2.time.dt.year == 2020)).where(land_mask)\n\nsel_le_JJA=uxds_fine2.slhf.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020])))).where(land_mask)/3600 #convert back to W/m2\nsel_tp_JJA=uxds_fine2.tp.sel(time=(uxds_fine2.time.dt.month.isin([6,7,8])&(uxds_fine2.time.dt.year.isin([2020])))).where(land_mask)\n\ndaily_sel_le_DJF=sel_le_DJF.resample(time='1D').mean()\ndaily_sel_tp_DJF=sel_tp_DJF.resample(time='1D').mean()\n\ndaily_sel_le_JJA=sel_le_JJA.resample(time='1D').mean()\ndaily_sel_tp_JJA=sel_tp_JJA.resample(time='1D').mean()\n\ndaily_sel_le_DJF.shape,daily_sel_le_JJA.shape\n\nfrom scipy.stats import rankdata\n\nts1_array = daily_sel_le_JJA.values\nts2_array = daily_sel_tp_JJA.values\n\nn_face = ts1_array.shape[1]\ncorrs_JJA = np.full(n_face, np.nan)\n\nfor i in range(n_face):\n    ts1 = ts1_array[:, i]\n    ts2 = ts2_array[:, i]\n\n    valid = ~np.isnan(ts1) & ~np.isnan(ts2)\n    if np.sum(valid) > 2:\n        r1 = rankdata(ts1[valid])\n        r2 = rankdata(ts2[valid])\n        corrs_JJA[i] = np.corrcoef(r1, r2)[0, 1]\n\nnp.nanmin(corrs_JJA),np.nanmean(corrs_JJA),np.nanmax(corrs_JJA)\n\ncorrs_JJA_ux=ux.UxDataArray(corrs_JJA.astype(float),uxgrid=uxds_fine2.uxgrid,dims=land_mask.dims)\n\nts1_array = daily_sel_le_DJF.values\nts2_array = daily_sel_tp_DJF.values\n\nn_face = ts1_array.shape[1]\ncorrs_DJF = np.full(n_face, np.nan)\n\nfor i in range(n_face):\n    ts1 = ts1_array[:, i]\n    ts2 = ts2_array[:, i]\n\n    valid = ~np.isnan(ts1) & ~np.isnan(ts2)\n    if np.sum(valid) > 2:\n        r1 = rankdata(ts1[valid])\n        r2 = rankdata(ts2[valid])\n        corrs_DJF[i] = np.corrcoef(r1, r2)[0, 1]\n\nnp.nanmin(corrs_DJF),np.nanmean(corrs_DJF),np.nanmax(corrs_DJF)\n\ncorrs_DJF_ux=ux.UxDataArray(corrs_DJF.astype(float),uxgrid=uxds_fine2.uxgrid,dims=land_mask.dims)\n\ncorrs_DJF_ux.plot()\n\ncorrs_JJA_ux.plot()\n\ncorrs_ux.plot()","type":"content","url":"/calc-le-prcp-corr#basic-correlation","position":11},{"hierarchy":{"lvl1":"HEALPIX FOR REGIONAL DATA LIKE CONUS DOESN’T WORK (YET)"},"type":"lvl1","url":"/healpix-wrf-conus-wind","position":0},{"hierarchy":{"lvl1":"HEALPIX FOR REGIONAL DATA LIKE CONUS DOESN’T WORK (YET)"},"content":"import intake\nimport uxarray as ux\nimport cartopy.crs as ccrs\nimport healpy\nimport cmocean\nimport matplotlib.pyplot as plt\nimport easygems.healpix as egh\nimport cartopy.feature as cf\nimport time\n\ncat_url = \"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"\ncat = intake.open_catalog(cat_url)['NCAR']\n\n\nmodel_run = cat.wrf_conus\n\n%%time\nds= model_run(zoom=10).to_dask()\na=ds['eastward_wind'][:744]\na\n\n\n%%time\nds\n\n# egh.healpix_show(ds.eastward_wind.isel(Time=0), ax=ax, cmap=cmocean.cm.thermal)\n\nzoom_level=[7,8,9,10]\ntime_zoom=[]\nprojection = ccrs.Robinson(central_longitude=-100)\n\nfor z in zoom_level:\n    start_time=time.time()\n    ds= model_run(zoom=z).to_dask()\n    a=ds['eastward_wind'].cell.values\n    end_time=time.time()\n    total_time=end_time-start_time\n    time_zoom.append(total_time)\n\n\n    fig, ax = plt.subplots(\n        figsize=(8, 4), subplot_kw={\"projection\": projection}, constrained_layout=True\n    )\n    ax.set_extent([-135, -60, 20, 55], crs=ccrs.PlateCarree())\n    \n    egh.healpix_show(ds.eastward_wind.isel(Time=0), ax=ax, cmap=cmocean.cm.thermal)\n    ax.add_feature(cf.COASTLINE, linewidth=0.8)\n    ax.add_feature(cf.BORDERS, linewidth=0.4)\n    plt.show()\n\nplt.plot(zoom_level,time_zoom)\nplt.xlabel('Zoom Level')\nplt.ylabel('Time in seconds')\nplt.title('Time vs Zoom')\n\nds.eastward_wind.isel(Time=0)\n\n","type":"content","url":"/healpix-wrf-conus-wind","position":1},{"hierarchy":{"lvl1":"HEALPIX FOR REGIONAL DATA LIKE CONUS DOESN’T WORK (YET)"},"type":"lvl1","url":"/healpix-wrf-conus-wind#healpix-for-regional-data-like-conus-doesnt-work-yet","position":2},{"hierarchy":{"lvl1":"HEALPIX FOR REGIONAL DATA LIKE CONUS DOESN’T WORK (YET)"},"content":"\n\nuxds = ux.UxDataset.from_healpix(ds)\n# uxds=uxds.eastward_wind\nuxds\n\nuxds2 = uxds.drop_vars(\"cell\")\n\nuxds2\n\nuxda = uxds.isel(Time=10)\n\nuxda.plot()\n\nuxds2.isel(Time=10)","type":"content","url":"/healpix-wrf-conus-wind#healpix-for-regional-data-like-conus-doesnt-work-yet","position":3},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/how-to-cite","position":1},{"hierarchy":{"lvl1":"Check on urban-specific urban signals"},"type":"lvl1","url":"/obs-urban-impact-on-coupling","position":0},{"hierarchy":{"lvl1":"Check on urban-specific urban signals"},"content":"","type":"content","url":"/obs-urban-impact-on-coupling","position":1},{"hierarchy":{"lvl1":"Check on urban-specific urban signals"},"type":"lvl1","url":"/obs-urban-impact-on-coupling#check-on-urban-specific-urban-signals","position":2},{"hierarchy":{"lvl1":"Check on urban-specific urban signals"},"content":"Author: Yifan Cheng\n\nimport datetime\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\nprint(f'Last updated at {current_time}')\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\nimport hvplot\nfrom pathlib import Path \nimport xarray as xr\nimport cartopy.crs as ccrs\nimport uxarray as ux\nimport cartopy.feature as cf\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport intake\n\ncat_url = \"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"\ncat = intake.open_catalog(cat_url).NCAR\n\n# check on global models\nmodel_run1 = cat['icon_d3hp003']\nmodel_run2 = cat['ifs_tco3999-ng5_deepoff']\nmodel_run3 = cat['nicam_gl11']\nmodel_run4 = cat['mpas_dyamond3']\nmodel_run5 = cat['mpas_dyamond2']\nmodel_run6 = cat['scream_ne120'] #3hourly average\nmodel_run7 = cat['wrf_conus']\n\nmodel_run5.describe()['user_parameters']\n\nds_fine5 = model_run5(zoom=10,time='PT3H').to_dask()\n\nuxds_fine5 = ux.UxDataset.from_healpix(ds_fine5)\n\nland_mask=ux.UxDataArray((uxds_fine5.vegfra.isel(time=0)!=0).astype(int),uxgrid=uxds_fine5.uxgrid)\n\n%%time\nuxds_fine5_skintemp=uxds_fine5.skintemp\n\n# Chicago, IL\nchi_bounds = (41.6 - 1.5, 42.1 + 1.5), (-88.0 - 1.5, -87.5 + 1.5)\n# Phoenix, AZ\nphx_bounds = (33.2 - 1, 33.7 + 1), (-112.3 - 1, -111.8 + 1)\n# Denver, CO\ndnv_bounds = (39.5 - 0.5, 40.1 + 0.5), (-105.2 - 0.5, -104.5 + 0.5)\n# Los Angeles, CA\nlax_bounds = (33.7 - 1.5, 34.3 + 1.5), (-118.7 - 1.5, -118.0 + 1.5)\n# Portland, OR\npor_bounds = (45.4 - 1, 45.7 + 1), (-123.1 - 1, -122.4 + 1)\n# Baltimore, MD\nbal_bounds = (39.2 - 1, 39.4 + 1), (-76.8 - 1, -76.4 + 1)\n# Miami, FL\nmia_bounds = (25.4 - 1, 26.5 + 1), (-80.5 - 1, -80 + 1)\n# Boston, MA\nbos_bounds = (42.2 - 1.5, 42.4 + 1.5), (-71.2 - 1.5, -70.9 + 1.5)\n# Dallas, TX\ndal_bounds = (32.6 - 1, 33.1 + 1), (-97.1 - 1, -96.6 + 1)\n# Atlanta, GA\natl_bounds = (33.6 - 1.5, 34.1 + 1.5), (-84.6 - 1.5, -84.2 + 1.5)\n\n%%time\nlat_bounds, lon_bounds = dal_bounds\n\nuxda_fine_subset = uxds_fine5_skintemp.subset.bounding_box(lon_bounds, lat_bounds)\n\nuxda_fine_subset.sel(time='2020-02-28').min().values\n\nuxda_fine_subset.sel(time='2020-02-28').max().values\n\ntimes_UCT=['2020-02-28T06:00:00','2020-02-28T09:00:00','2020-02-28T12:00:00',\n            '2020-02-28T15:00:00','2020-02-28T18:00:00','2020-02-28T21:00:00',\n            '2020-02-29T00:00:00','2020-02-29T03:00:00']\n\ntimes_CDT=['2020-02-28 01:00:00','2020-02-28 04:00:00','2020-02-28 07:00:00',\n            '2020-02-28 10:00:00','2020-02-28 13:00:00','2020-02-28 16:00:00',\n            '2020-02-28 19:00:00','2020-02-28 22:00:00']\n\ntimes_MDT =['2020-02-28 00:00:00', '2020-02-28 03:00:00', '2020-02-28 06:00:00',\n '2020-02-28 09:00:00', '2020-02-28 12:00:00', '2020-02-28 15:00:00',\n '2020-02-28 18:00:00', '2020-02-28 21:00:00']\n\ntimes_PDT =['2020-02-27 23:00:00', '2020-02-28 02:00:00', '2020-02-28 05:00:00',\n '2020-02-28 08:00:00', '2020-02-28 11:00:00', '2020-02-28 14:00:00',\n '2020-02-28 17:00:00', '2020-02-28 20:00:00']\n\ntimes_EDT =['2020-02-28 02:00:00', '2020-02-28 05:00:00', '2020-02-28 08:00:00',\n '2020-02-28 11:00:00', '2020-02-28 14:00:00', '2020-02-28 17:00:00',\n '2020-02-28 20:00:00', '2020-02-28 23:00:00']\n\nhvplot.extension('bokeh')\n\nshp = gpd.read_file(\"/glade/work/yifanc17/02_data/02_urban_1km_dataset/boundary_shp/USGS_TIGER_city/Dallas.shp\")\n\ni=7\nheatmap=uxda_fine_subset.sel(time=times_UCT[i]).plot(\n    backend='bokeh',\n    cmap=\"coolwarm\",\n    title=f\"Dallas, TX @{times_CDT[i]} CDT\",\n    features=[\"coastline\", \"borders\"],\n    clim=(280,310)\n)\n\nboundary = shp.hvplot(geo=True,\n    line_color='black',\n    line_width=1.5,\n    color='None',  \n    alpha=1)\n\ncombined = (heatmap * boundary).opts(\n    width=660,\n    height=600,\n    framewise=True,\n    aspect='equal',\n    xlim=(-97.1 - 0.7, -96.6 + 0.5),\n    ylim=(32.6 - 0.25, 33.1 + 0.2),\n    fontsize={'title': 20, 'labels': 12, 'xticks': 10, 'yticks': 10}\n)\n\ncombined\n\ndef average_temp_in_city(tskin, city_shapefile):\n    city_geom = city_shapefile.unary_union\n\n    lats = tskin.uxgrid.face_lat.values\n    lons = tskin.uxgrid.face_lon.values\n    points = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n\n    inside_mask = np.array([city_geom.contains(p) for p in points])\n\n    temp_values = tskin.values\n    city_mean_temp = np.nanmean(temp_values[inside_mask])\n\n    return city_mean_temp\n\nimport matplotlib.animation as animation\nfrom PIL import Image\n\nlat_bounds, lon_bounds = dal_bounds\nuxda_fine_subset = uxds_fine5_skintemp.subset.bounding_box(lon_bounds, lat_bounds)\nshp = gpd.read_file(\"/glade/work/yifanc17/02_data/02_urban_1km_dataset/boundary_shp/USGS_TIGER_city/Dallas.shp\")\n\nimage_files = [\n    'dal_20200228_1am.png', 'dal_20200228_4am.png', 'dal_20200228_7am.png',\n    'dal_20200228_10am.png', 'dal_20200228_1pm.png', 'dal_20200228_4pm.png',\n    'dal_20200228_7pm.png', 'dal_20200228_10pm.png'\n]\n\ntemps=[]\nfor time in ['2020-02-28T06:00:00','2020-02-28T09:00:00','2020-02-28T12:00:00',\n            '2020-02-28T15:00:00','2020-02-28T18:00:00','2020-02-28T21:00:00',\n            '2020-02-29T00:00:00','2020-02-29T03:00:00']:   \n    value=average_temp_in_city(uxda_fine_subset.sel(time=time),shp)\n    temps.append(value)\n\ntimes = ['1am','4am','7am','10am','1pm','4pm','7pm','10pm']\n\nfig, ax = plt.subplots(figsize=(10, 10), dpi=300)\nimg_handle = ax.imshow(np.zeros((10, 10, 3), dtype=np.uint8)) \nline_ax = fig.add_axes([0.21, 0.18, 0.15, 0.15]) \nline_plot, = line_ax.plot([], [], 'r-o', lw=2)\n\ndef update(i):\n    img = np.array(Image.open(image_files[i]))\n    img_handle.set_data(img)\n    ax.axis('off') \n\n    line_ax.clear()\n    line_ax.plot(times[:i+1], temps[:i+1], color='dimgray', marker='o', markersize=4, lw=2)\n    line_ax.set_xlim(0, len(temps))\n    line_ax.set_ylim(min(temps) - 1, max(temps) + 1)\n    line_ax.set_title(\"Diurnal Temp (K)\", fontsize=8)\n    line_ax.set_xticklabels([])\n    line_ax.set_yticklabels([])\n\n    return img_handle, line_plot\n\nani = animation.FuncAnimation(fig, update, frames=len(image_files), blit=False)\n\nani.save(\"dal_diurnal_line_overlay.mov\", fps=5, writer=\"ffmpeg\",bitrate=8000)\n\n","type":"content","url":"/obs-urban-impact-on-coupling#check-on-urban-specific-urban-signals","position":3},{"hierarchy":{"lvl1":"Bowen ratio"},"type":"lvl1","url":"/obs-urban-impact-on-coupling#bowen-ratio","position":4},{"hierarchy":{"lvl1":"Bowen ratio"},"content":"\n\n%%time\nuxds_fine1_bowen=uxds_fine1.hfssd.sel(time='2020-02-29')/uxds_fine1.hflsd.sel(time='2020-02-29')\n\n%%time\nuxds_fine1_bowen2=uxds_fine1.hfssd.sel(time='2020-08-01')/uxds_fine1.hflsd.sel(time='2020-08-01')\n\nuxds_fine1_bowen=uxds_fine1_bowen.where(np.isfinite(uxds_fine1_bowen))\n\nuxds_fine1_bowen2=uxds_fine1_bowen2.where(np.isfinite(uxds_fine1_bowen2))\n\n%%time\n# denver CO\nlat_bounds = (39.6-1, 39.9+1)\nlon_bounds = (-105.1-1, -104.7+1)\n\nuxda_fine_subset = uxds_fine1_bowen.subset.bounding_box(lon_bounds, lat_bounds)\n#uxda_fine_subset = uxds_fine1.ts.sel(time='2020-02-29T03:00:00.000000000').subset.bounding_box(lon_bounds, lat_bounds)\n\n%%time\n# Chicago, IL\nlat_bounds = (41.6-1, 42.1+1)\nlon_bounds = (-88.0-1, -87.5+1)\n\nuxda_fine_subset2 = uxds_fine1_bowen.subset.bounding_box(lon_bounds, lat_bounds)\n#uxda_fine_subset2 = uxds_fine1.ts.sel(time='2020-02-29T03:00:00.000000000').subset.bounding_box(lon_bounds, lat_bounds)\n\n%%time\nimport geopandas as gpd\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nimport hvplot\n\nhvplot.extension('bokeh')\n\nshp = gpd.read_file(\"/glade/work/yifanc17/02_data/02_urban_1km_dataset/boundary_shp/USGS_TIGER_city/Chicago.shp\")\n\nheatmap=uxda_fine_subset.plot(\n    backend='bokeh',\n    cmap=\"viridis\",\n    title=\"MPAS_DYAMOND2 2020-02-29 T03:00:00\",\n    features=[\"coastline\", \"borders\"]\n)\n\nboundary = shp.hvplot(geo=True,\n    line_color='black',\n    line_width=1.5,\n    color='None',  \n    alpha=1)\n\n\ncombined = (heatmap * boundary).opts(\n    width=650,\n    height=600,\n)\n\ncombined\n\nhvplot.extension('bokeh')\n\nshp = gpd.read_file(\"/glade/work/yifanc17/02_data/02_urban_1km_dataset/boundary_shp/CHI/Chicago.shp\")\n\nheatmap=uxda_fine_subset2.plot(\n    cmap='viridis',\n    backend='bokeh',\n    title=\"icon_d3hp003 2020-02-29\",\n    features=[\"coastline\", \"borders\"],\n    clim=(0,2)\n)\n\nboundary = shp.hvplot(geo=True,\n    line_color='black',\n    line_width=1.5,\n    color='None',  \n    alpha=1)\n\n\ncombined = (heatmap * boundary).opts(\n    width=1000,\n    height=600,\n)\n\ncombined","type":"content","url":"/obs-urban-impact-on-coupling#bowen-ratio","position":5},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/read-visualize-wind-idx","position":0},{"hierarchy":{"lvl1":""},"content":"import uxarray as ux\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport os,sys\nimport OpenVisus as ov\nimport openvisuspy as ovp\nimport xarray as xr\nfrom tqdm import tqdm\nimport time\nfrom sys import getsizeof\nimport cmocean\nimport easygems.healpix as egh\n\n\n# !pip install OpenVisus openvisuspy boto3 xmltodict colorcet\n\nidx_filename=f\"./idx/conus404_eastward_wind.idx\"\n\ndef get_size_in_mb(variable):\n  \"\"\"\n    Calculates the size of a variable in megabytes (MB).\n\n    Args:\n        variable: The variable to check the size of.\n\n    Returns:\n        The size of the variable in MB.\n  \"\"\"\n  size_in_bytes = sys.getsizeof(variable)\n  size_in_mb = size_in_bytes / (1024 * 1024)\n  return size_in_mb\n\n\ndb=ov.LoadDataset(idx_filename)\n\ntime_to_read_idx=[]\nquality=[0,-2,-4,-6,-8,-10,-12,-14,-16]\nfig, axs = plt.subplots(1, 3, figsize=(18, 3))\nfor ax,q in zip(axs,quality[0:3]):\n    start_time=time.time()\n    data=db.read(quality=q)\n    end_time=time.time()\n    total_time=end_time-start_time\n    time_to_read_idx.append(total_time)\n    ax.imshow(data,origin='lower',cmap=cmocean.cm.thermal,aspect='auto')\n    ax.set_title(f'Quality level: {q}   Data Shape: {data.shape}')\nplt.show()\n\nprint(len(time_to_read_idx))\nfig, axs = plt.subplots(1, 3, figsize=(18, 3))\nfor ax,q in zip(axs,quality[3:6]):\n    start_time=time.time()\n    data=db.read(quality=q)\n    end_time=time.time()\n    total_time=end_time-start_time\n    time_to_read_idx.append(total_time)    \n    ax.imshow(data,origin='lower',cmap=cmocean.cm.thermal,aspect='auto')\n    ax.set_title(f'Quality level: {q}   Data Shape: {data.shape}')\nplt.show()\nfig, axs = plt.subplots(1, 3, figsize=(18, 3))\nfor ax,q in zip(axs,quality[6:9]):\n    start_time=time.time()\n    data=db.read(quality=q)\n    end_time=time.time()\n    total_time=end_time-start_time\n    time_to_read_idx.append(total_time)    \n    ax.imshow(data,origin='lower',cmap=cmocean.cm.thermal,aspect='auto')\n    ax.set_title(f'Quality level: {q}   Data Shape: {data.shape}')\nplt.show()\n\n\nplt.plot(quality,time_to_read_idx)\nplt.xlabel('Quality Level')\nplt.ylabel('Time in seconds')\nplt.title('Time vs quality')\n\nprojection = ccrs.Robinson(central_longitude=-100)\nfig, ax = plt.subplots(\n    figsize=(8, 4), subplot_kw={\"projection\": projection}, constrained_layout=True\n)\nax.set_extent([-135, -60, 20, 55], crs=ccrs.PlateCarree())\n\negh.healpix_show(data, ax=ax, cmap=cmocean.cm.thermal)\nax.add_feature(cf.COASTLINE, linewidth=0.8)\nax.add_feature(cf.BORDERS, linewidth=0.4)","type":"content","url":"/read-visualize-wind-idx","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/reencode-wrf-conus","position":0},{"hierarchy":{"lvl1":""},"content":"import uxarray as ux\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport os\nimport OpenVisus as ov\nimport xarray as xr\nfrom tqdm import tqdm\nimport time\nfrom sys import getsizeof\n\n# !pip install OpenVisus openvisuspy boto3 xmltodict panel bokeh\n# !pip install tqdm\n\nmain_dir='/glade/derecho/scratch/digital-earths-hackathon/conus404/native_grid'\nhome_dir='/glade/u/home/dpanta'\nidx_dir='/glade/u/home/dpanta/idx'\n\nst_file='/glade/derecho/scratch/digital-earths-hackathon/conus404/native_grid/2020-10_eastward_wind.nc'\n\n\nfiles=[\n'2020-12_eastward_wind.nc',\n'2020-12_northward_wind.nc',\n'2020-12_precipitation_flux.nc',\n'2020-12_specific_humidity.nc',\n'2020-12_surface_air_pressure.nc',\n'2020-12_surface_temperature.nc',\n'2020-12_toa_outgoing_longwave_flux.nc',\n'2020-12_toa_outgoing_shortwave_flux.nc',\n]\n\nds=xr.open_dataset(st_file)\nds\n\nimport datetime\n\nidx_filename=f\"/glade/derecho/scratch/dpanta/idx/conus404_all.idx\"\nfields=['eastward_wind','northward_wind','precipitation_flux','specific_humidity','surface_air_pressure','surface_temperature','toa_outgoing_longwave_flux', 'toa_outgoing_shortwave_flux']\narco=\"4mb\"\nfld_idx=[]\nfor fld in fields:\n    print(f'Setting Field::: {fld}')\n    st_file=f'/glade/derecho/scratch/digital-earths-hackathon/conus404/native_grid/2020-10_{fld}.nc'\n\n    ds = xr.open_dataset(st_file, group=\"/\", mask_and_scale=False) \n    data=ds[f\"{fld}\"][0,:,:].values\n    if int(len(data.shape))>1:\n        \n        print(f\"{fld}::::{data.shape}\")\n        try:\n            vmin,vmax=np.min(data),np.max(data)\n        except:\n            vmin,vmax=0,0\n        fld=ov.Field.fromString(f\"\"\"{fld} {str(data.dtype)} format(row_major) min({vmin}) max({vmax})\"\"\")\n        fld_idx.append(fld)\n\nT=744\nW,H=data.shape\n\n\nprint('Creating IDX Metadata file now...')\ndb=ov.CreateIdx(\n    url=idx_filename, \n\tdims=[H,W], \n\tfields=fld_idx, \n\tcompression=\"raw\", \n\tarco=arco,\n\ttime=[0,T,\"time_%d/\"])\nprint('Created...')\n\n# db=ov.LoadDataset(idx_filename)\n\nprint('Writing to idx...')\nfor i in range(len(fields)):\n    print(f'Writing field {fields[i]}...')\n    st_file=f'/glade/derecho/scratch/digital-earths-hackathon/conus404/native_grid/2020-10_{fields[i]}.nc'\n    ds = xr.open_dataset(st_file, group=\"/\", mask_and_scale=False) \n    data=ds[f\"{fields[i]}\"][...].values\n    for t in range(data.shape[0]):\n        data_to_write=data[t,:,:]\n        try:\n            db.write(data_to_write,time=t,field=fields[i])\n        except:\n            pass\nprint('Writing data completed...')\nprint('Compressing data now...')\ndb.compressDataset(['zip'])\nprint('Data compressed!!!')","type":"content","url":"/reencode-wrf-conus","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/soilmoisture-precipicon-2","position":0},{"hierarchy":{"lvl1":""},"content":"import intake\nimport uxarray as ux\nimport cartopy.crs as ccrs\nimport geoviews.feature as gf\nimport holoviews as hv\n\nimport healpy as hp\nimport numpy as np\n\n#Calculates the effect of soil moisture anomalies on afternoon precipitation using the ICON dataset for 2020. Inspired by Welty et al 2019 observational study.\n#ERIK JANZON\n#UNIVERSITY OF NEBRASKA-LINCOLN\n\n#DIGITAL EARTHS HACKATHON\n#NCAR NODE\n#MAY 2025\n\n#MAKE SURE THE CHANGE THE CONDITIONS BELOW (CELL 42, LINE 36-38) TO REFLECT THE SEASON/HOURS YOU WANT TO LOOK AT \n\n#MAKE SURE TO ADJUST PLOT TITLES\n\n#Issues:\n#1.) Only figured out how to look at one time zone at a time, so \"afternoon\" is based on CDT in this case. Maybe not a problem since the data is only \n#    3 hourly?\n#2.) Cannot calculate moisture convergence. Tried to use uxarray.gradient(), but no luck because one cannot remap to grid cell faces for comparison.\n#\n#3.) Only one year of data in ICON. Longer model datasets missing soil moisture (unless I missed something...)\n\n#4.) Welty et al 2019 performed more useful statistics. See Yifang Cheng's notebook for example of probabilities.\n\n#5.) HARD CODED STUFF:\n    #a.) MAKE SURE THE CHANGE THE CONDITIONS BELOW (CELL 42, LINE 36-38) TO REFLECT THE SEASON/HOURS YOU WANT TO LOOK AT \n\n    #b.) MAKE SURE TO ADJUST PLOT TITLES\n\nzoom=8\n\n#UTC offset.\noffset=6;\n\n#SUBSETS FOR DIFFERENT AREAS.\n#CONUS\nlon_bounds = (-130, -50)\nlat_bounds = (20, 60)\n\n#Central South America\n#lon_bounds = (-90, -30)\n#lat_bounds = (-40, 0)\n\n#Southern Africa\n#lon_bounds = (0, -40)\n#lat_bounds = (-30, 0)\n\nfrom datetime import datetime\n\n#Set up data fetch and list catalogue\n\nurl=\"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"\n\ncat = intake.open_catalog(url).NCAR\n\nlist(cat)\n\n#Start with mpas_dyamond1\n\nmodel_run=cat.icon_d3hp003\nmodel_run().describe()\n\n#Play with lower zoom level, fetching data. Increase zoom later.\n\nds=model_run(zoom=zoom,time='PT3H').to_dask()\nuxds=ux.UxDataset.from_healpix(ds)\ntimes=uxds.time\n\nhours=uxds.time.dt.hour\nuxds\n\n\n#Create subsets of soil moisture and precipitation for northern hemisphere south of 60 degrees north across continents\n\n#lon_bounds = (-130, -50)\n#lat_bounds = (20, 60)\n\n#lon_bounds = (-90, -30)\n#lat_bounds = (-40, 0)\n\n#qv=uxds[\"ie\"].subset.bounding_box(\n#    lon_bounds,\n#    lat_bounds,\n#)\n\nuxsmois = uxds[\"mrso\"].subset.bounding_box(\n    lon_bounds,\n    lat_bounds,\n)\n\nuxrain = uxds[\"pr\"].subset.bounding_box(\n    lon_bounds,\n    lat_bounds,\n)\n\n#conditionafternoon=((uxrain.time.dt.hour >= 12+6) & (uxrain.time.dt.hour <= 17+6))\n\n#conditionmorning=((uxrain.time.dt.hour >= 5+6) & (uxrain.time.dt.hour < 12+6))\n\n#conditionafternoon=((uxrain.time.dt.month >= 12) & (uxrain.time.dt.month <= 2) & (uxrain.time.dt.hour >= 12+offset) & (uxrain.time.dt.hour <= 17+offset))\n\n#conditionmorning=((uxrain.time.dt.month >= 12) & (uxrain.time.dt.month <= 2) & (uxrain.time.dt.hour >= 5+offset) & (uxrain.time.dt.hour < 12+offset))\n#wspd=((u**2)+(v**2))**(1/2)\n#qvu=qv*wspd\n\n\n#MAKE SURE TO CHANGE THIS WHEN LOOKING AT NORTH/SOUTH HEMISPHERE!!!!!!\nconditionafternoon=(((uxrain.time.dt.month >= 6) | (uxrain.time.dt.month <= 8)) & (uxrain.time.dt.hour >= 12+offset) & (uxrain.time.dt.hour <= 17+offset))\n\nconditionmorning=(((uxrain.time.dt.month >= 6) | (uxrain.time.dt.month <= 8)) & (uxrain.time.dt.hour >= 5+offset) & (uxrain.time.dt.hour < 12+offset))\n\n\n\n\nuxrain_aft=uxrain[conditionafternoon]\nuxrain_mor=uxrain[conditionmorning]\nuxsmois_mor=uxsmois[conditionmorning]\n#qv_mor=qv[conditionmorning]\nuxrain_aft.time\n\n\n\nplot_opts = {\"width\": 700, \"height\": 350}\n\n#uxrain_mean_aft=uxrain_aft.sum(dim=\"nVertLevels\") \n\n#uxrain_mean_mor=uxrain_mor.sum(dim=\"nVertLevels\") \n\nuxrain_mean_mor=uxrain_mor\n\nuxrain_mean_aft=uxrain_aft\n\nuxsmois_mean_mor=uxsmois_mor.mean(dim=\"soil_level\") \n#uxsmois_mean_mor=uxsmois_mean_mor\n\n#uxsmois_mean_mor=uxsmois_mor.mean(dim=\"soil_level\") \nuxrain_plot=uxrain_mean_aft.mean(dim=\"time\")\n\n\n\nfeatures = gf.coastline(\n    projection=ccrs.PlateCarree(), line_width=1, scale=\"50m\"\n) * gf.states(projection=ccrs.PlateCarree(), line_width=1, scale=\"50m\")\n\nuxrain_plot.plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    title=\"Mean Afternoon Total Column Rain Mixing Ratio, JJA, ERA5\",\n    **plot_opts,\n) * features\n\n# Condition: values greater than 0.5\n#uxrain_max=uxrain_mean_aft.max(dim=\"n_face\")\n\n\n\n\n# Apply the condition to get the values\n#sum_hours_rain=uxrain_mean_aft.groupby_bins(\"n_face\",[.01e-6,100]).groups\n#sum_hours_rain\n\ndaily_rain=uxrain_mean_aft.resample(time='D')\n\ndaily_mor_rain=uxrain_mean_mor.resample(time='D')\ndaily_smois=uxsmois_mean_mor.resample(time='D')\n#dailyqv=qv_mor.resample(time='D')\n#condition = daily_rain > 0.01e-6\n\n#uxrain1=daily_rain.where(condition).sum(dim=\"time\")\nres_rain=daily_rain.sum()\nres_mor_rain=daily_mor_rain.sum()\nres_smois=daily_smois.sum()\n#res_qv=dailyqv.mean()\n\nres_smois.mean(dim=\"time\").plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    title=\"Mean Total Soil Moisture (sum of layers), Morning\",\n    **plot_opts,\n) * features\n\nagg_rain=res_rain.where((res_mor_rain<0.02e-5) & (res_rain>1e-5))\nagg_rain_smois=agg_rain.where((res_smois>0.1) & (res_smois<=1)).count(dim=\"time\")\nagg_rain_smois.plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    clim=(1,15),\n    title=\"Afternoon Rainfall, with no morning rainfall\",\n    **plot_opts,\n) * features\n\n#Soil moisture anomalies\n#Remove water values\n\nsmois_valid=res_smois.where((res_smois<=1))\n\nsmois_anom=smois_valid-smois_valid.mean(dim=\"time\")\n\nmean_smois_anom_rain=smois_anom.where((res_mor_rain<0.02e-5) & (res_rain>1e-5)).mean(dim=\"time\")\n\nmean_smois_anom_rain.plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    title=\"JJA ICON Morning Soil Moisture Anomaly (with Afternoon Rainfall)\",\n    **plot_opts,\n) * features","type":"content","url":"/soilmoisture-precipicon-2","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/soilmoisture-precip-mpas","position":0},{"hierarchy":{"lvl1":""},"content":"import intake\nimport uxarray as ux\nimport cartopy.crs as ccrs\nimport geoviews.feature as gf\nimport holoviews as hv\n\nimport healpy as hp\nimport numpy as np\n\nfrom datetime import datetime\n\n\nimport xarray as xr\n#Calculates the effect of soil moisture anomalies on afternoon precipitation using the MPAS Dyamond 1 dataset for August 2016. Inspired by Welty et al 2019.\n#ERIK JANZON\n#UNIVERSITY OF NEBRASKA-LINCOLN\n\n#DIGITAL EARTHS HACKATHON\n#NCAR NODE\n#MAY 2025\n\n#Issues:\n#1.) Only figured out how to look at one time zone at a time, so \"afternoon\" is based on CDT in this case. Maybe not a problem since the data is only \n#    3 hourly?\n#2.) Cannot calculate moisture convergence. Tried to use uxarray.gradient(), but no luck because one cannot remap to grid cell faces for comparison.\n#\n#3.) Only MPAS run that has all the needed variables is the Dyamond1 global run, which only has one month of data. Results look neat, but are likely not significant\n\n\n\n\n#Function to remap data onto LatLon Grid (unfinished, based on https://easy.gems.dkrz.de/Processing/healpix/lonlat_remap.html)\n#Ultimate goal is to back out structured grid to calculate gradients and, eventually, moisture convergence\n\ndef get_nn_lon_lat_index(nside, lons, lats):\n    lons2, lats2 = np.meshgrid(lons, lats)\n    return xr.DataArray(\n        healpy.ang2pix(nside, lons2, lats2, nest=True, lonlat=True),\n        coords=[(\"lat\", lats), (\"lon\", lons)],\n    )\nzoom=8\n#Set up data fetch and list catalogue\n\nurl=\"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"\n\ncat = intake.open_catalog(url).NCAR\n\n#Probably can be commented out. List catalog.\n#list(cat)\n\n#Start with mpas_dyamond1\n\nmodel_run=cat.mpas_dyamond1\n#Probably can be commented out. Reporting the available data in each model dataset.\n#model_run().describe()\n\n#Play with lower zoom level, fetching data. Increase zoom later.\n\nds=model_run(zoom=zoom).to_dask()\nuxds=ux.UxDataset.from_healpix(ds)\ntimes=uxds.time\n\nhours=uxds.time.dt.hour\nuxds\n\n#Create subsets of meteorological variables for North America\n\nlon_bounds = (-130, -50)\nlat_bounds = (20, 60)\n\nqv=uxds[\"qv\"].subset.bounding_box(\n    lon_bounds,\n    lat_bounds,\n)\n\nu=uxds[\"uReconstructZonal\"].subset.bounding_box(\n    lon_bounds,\n    lat_bounds,\n)\n\nv=uxds[\"uReconstructMeridional\"].subset.bounding_box(\n    lon_bounds,\n    lat_bounds,\n)\nqv=qv[:,:,0]\nu=u[:,:,0]\nv=v[:,:,0]\n\n\nuxsmois = uxds[\"smois\"].subset.bounding_box(\n    lon_bounds,\n    lat_bounds,\n)\n\nuxrain = uxds[\"qr\"].subset.bounding_box(\n    lon_bounds,\n    lat_bounds,\n)\n#Only look at afternoon hours for precipitation and morning hours to filter out low precipitation mornings and for soil moisture\nconditionafternoon=(uxrain.time.dt.hour >= 12+6) & (uxrain.time.dt.hour <= 17+6)\n\nconditionmorning=(uxrain.time.dt.hour >= 5+6) & (uxrain.time.dt.hour < 12+6)\n#wspd=((u**2)+(v**2))**(1/2)\n#qvu=qv*wspd\n\n\n#dqu_dx = qv.gradient()  # very rough gradient using wind speed. Probably doesn't work. But placeholder.\n\n\n\nuxrain_aft=uxrain[conditionafternoon]\nuxrain_mor=uxrain[conditionmorning]\nuxsmois_mor=uxsmois[conditionmorning]\nqv_mor=qv[conditionmorning]\nuxrain_aft\n\n\n\nplot_opts = {\"width\": 700, \"height\": 350}\n\nuxrain_mean_aft=uxrain_aft.sum(dim=\"nVertLevels\") \n\nuxrain_mean_mor=uxrain_mor.sum(dim=\"nVertLevels\") \n\n\nuxsmois_mean_mor=uxsmois_mor.mean(dim=\"nSoilLevels\") \n\n#uxsmois_mean_mor=uxsmois_mor.mean(dim=\"soil_level\") \nuxrain_plot=uxrain_mean_aft.mean(dim=\"time\")\n\nfeatures = gf.coastline(\n    projection=ccrs.PlateCarree(), line_width=1, scale=\"50m\"\n) * gf.states(projection=ccrs.PlateCarree(), line_width=1, scale=\"50m\")\n\nuxrain_plot.plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    title=\"Mean Afternoon Total Column Rain Mixing Ratio, August 2016, MPAS Dyamond1\",\n    **plot_opts,\n) * features\n\n\n\n# Condition: values greater than 0.5\n#uxrain_max=uxrain_mean_aft.max(dim=\"n_face\")\n\n\n\n\n# Apply the condition to get the values\n#sum_hours_rain=uxrain_mean_aft.groupby_bins(\"n_face\",[.01e-6,100]).groups\n#sum_hours_rain\n\ndaily_rain=uxrain_mean_aft.resample(time='D')\n\ndaily_mor_rain=uxrain_mean_mor.resample(time='D')\ndaily_smois=uxsmois_mean_mor.resample(time='D')\ndailyqv=qv_mor.resample(time='D')\n#condition = daily_rain > 0.01e-6\n\n#uxrain1=daily_rain.where(condition).sum(dim=\"time\")\nres_rain=daily_rain.sum()\nres_mor_rain=daily_mor_rain.sum()\nres_smois=daily_smois.sum()\nres_qv=dailyqv.mean()\n\nres_smois.mean(dim=\"time\").plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    title=\"Mean Total Soil Moisture (sum of layers), Morning\",\n    **plot_opts,\n) * features\n\nagg_rain=res_rain.where((res_mor_rain<0.02e-3) & (res_rain>1e-3))\nagg_rain_smois=agg_rain.where((res_smois>0.4) & (res_smois<=1)).count(dim=\"time\")\nagg_rain_smois.plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    clim=(1,15),\n    title=\"Afternoon Rainfall, with no morning rainfall\",\n    **plot_opts,\n) * features\n\n#Soil moisture anomalies\n#Remove water values\n\nsmois_valid=res_smois.where((res_smois<=1))\n\nsmois_anom=smois_valid-smois_valid.mean(dim=\"time\")\n\nmean_smois_anom_rain=smois_anom.where((res_mor_rain<0.02e-3) & (res_rain>1e-3)).mean(dim=\"time\")\n\nmean_smois_anom_rain.plot(\n    rasterize=True,\n    periodic_elements=\"exclude\",\n    title=\"MPAS Morning Soil Moisture Anomaly (with Afternoon Rainfall) (Dyamond1)\",\n    **plot_opts,\n) * features\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/soilmoisture-precip-mpas","position":1}]}